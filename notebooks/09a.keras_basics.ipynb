{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of MLP\n",
    "- Objective: create vanilla neural networks (i.e., Multilayer perceptrons) for simple classification task with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Structures\n",
    "- Each MLP model is consisted of one input layer, several hidden layers, and one output layer\n",
    "- Number of neurons in each layer is not limited\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net.jpeg\" style=\"width: 300px\"/>\n",
    "<br>\n",
    "<center>**MLP with one hidden layer**</center>\n",
    "- Number of input neurons: 3\n",
    "- Number of hidden neurons: 4\n",
    "- Number of output neurons: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" style=\"width: 500px\"/>\n",
    "<br>\n",
    "<center>**MLP with two hidden layers**</center>\n",
    "- Number of input neurons: 3\n",
    "- Number of hidden neurons: (4, 4)\n",
    "- Number of output neurons: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for classification tasks\n",
    "- When the target (**y**) is discrete (categorical)\n",
    "- For loss function, cross-entropy is used and for evaluation metric, accuracy is commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = whole_data.data\n",
    "y_data = whole_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3, random_state = 7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "- Breast cancer dataset has total 569 data instances (212 malign, 357 benign instances)\n",
    "- 30 attributes (features) to predict the binary class (M/B)\n",
    "- Doc: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398, 30)\n",
      "(171, 30)\n",
      "(398,)\n",
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating a model\n",
    "- Same with regression model at the outset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Adding layers\n",
    "- Keras layers can be **added** to the model\n",
    "- Adding layers are like stacking lego blocks one by one\n",
    "- It should be noted that as this is a classification problem, sigmoid layer (softmax for multi-class problems) should be added\n",
    "- Doc: https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras model with two hidden layer with 10 neurons each \n",
    "model.add(Dense(10, input_shape = (30,)))    # Input layer => input_shape should be explicitly designated\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))                         # Hidden layer => only output dimension should be designated\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))                         # Hidden layer => only output dimension should be designated\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))                          # Output layer => output dimension = 1 since it is a binary classification\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is equivalent to the above code block\n",
    "# model.add(Dense(10, input_shape = (30,), activation = 'sigmoid'))\n",
    "# model.add(Dense(10, activation = 'sigmoid'))\n",
    "# model.add(Dense(10, activation = 'sigmoid'))\n",
    "# model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Model compile\n",
    "- Keras model should be \"compiled\" prior to training\n",
    "- Types of loss (function) and optimizer should be designated\n",
    "    - Doc (optimizers): https://keras.io/optimizers/\n",
    "    - Doc (losses): https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.01)    # stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 541\n",
      "Trainable params: 541\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "- Training the model with training data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278 samples, validate on 120 samples\n",
      "Epoch 1/100\n",
      "278/278 [==============================] - 0s - loss: 0.6926 - acc: 0.5612 - val_loss: 0.6890 - val_acc: 0.5833\n",
      "Epoch 2/100\n",
      "278/278 [==============================] - 0s - loss: 0.6867 - acc: 0.6151 - val_loss: 0.6875 - val_acc: 0.5833\n",
      "Epoch 3/100\n",
      "278/278 [==============================] - 0s - loss: 0.6833 - acc: 0.6151 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 4/100\n",
      "278/278 [==============================] - 0s - loss: 0.6801 - acc: 0.6151 - val_loss: 0.6824 - val_acc: 0.5833\n",
      "Epoch 5/100\n",
      "278/278 [==============================] - 0s - loss: 0.6769 - acc: 0.6151 - val_loss: 0.6804 - val_acc: 0.5833\n",
      "Epoch 6/100\n",
      "278/278 [==============================] - 0s - loss: 0.6742 - acc: 0.6151 - val_loss: 0.6776 - val_acc: 0.5833\n",
      "Epoch 7/100\n",
      "278/278 [==============================] - 0s - loss: 0.6715 - acc: 0.6151 - val_loss: 0.6754 - val_acc: 0.5833\n",
      "Epoch 8/100\n",
      "278/278 [==============================] - 0s - loss: 0.6694 - acc: 0.6151 - val_loss: 0.6741 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      "278/278 [==============================] - 0s - loss: 0.6672 - acc: 0.6151 - val_loss: 0.6729 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      "278/278 [==============================] - 0s - loss: 0.6654 - acc: 0.6151 - val_loss: 0.6725 - val_acc: 0.5833\n",
      "Epoch 11/100\n",
      "278/278 [==============================] - 0s - loss: 0.6642 - acc: 0.6151 - val_loss: 0.6720 - val_acc: 0.5833\n",
      "Epoch 12/100\n",
      "278/278 [==============================] - 0s - loss: 0.6631 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 13/100\n",
      "278/278 [==============================] - 0s - loss: 0.6623 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 14/100\n",
      "278/278 [==============================] - 0s - loss: 0.6619 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 15/100\n",
      "278/278 [==============================] - 0s - loss: 0.6611 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 16/100\n",
      "278/278 [==============================] - 0s - loss: 0.6609 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 17/100\n",
      "278/278 [==============================] - 0s - loss: 0.6603 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 18/100\n",
      "278/278 [==============================] - 0s - loss: 0.6600 - acc: 0.6151 - val_loss: 0.6713 - val_acc: 0.5833\n",
      "Epoch 19/100\n",
      "278/278 [==============================] - 0s - loss: 0.6597 - acc: 0.6151 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 20/100\n",
      "278/278 [==============================] - 0s - loss: 0.6593 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 21/100\n",
      "278/278 [==============================] - 0s - loss: 0.6591 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 22/100\n",
      "278/278 [==============================] - 0s - loss: 0.6589 - acc: 0.6151 - val_loss: 0.6702 - val_acc: 0.5833\n",
      "Epoch 23/100\n",
      "278/278 [==============================] - 0s - loss: 0.6591 - acc: 0.6151 - val_loss: 0.6702 - val_acc: 0.5833\n",
      "Epoch 24/100\n",
      "278/278 [==============================] - 0s - loss: 0.6588 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 25/100\n",
      "278/278 [==============================] - 0s - loss: 0.6585 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 26/100\n",
      "278/278 [==============================] - 0s - loss: 0.6582 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 27/100\n",
      "278/278 [==============================] - 0s - loss: 0.6582 - acc: 0.6151 - val_loss: 0.6704 - val_acc: 0.5833\n",
      "Epoch 28/100\n",
      "278/278 [==============================] - 0s - loss: 0.6582 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 29/100\n",
      "278/278 [==============================] - 0s - loss: 0.6580 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 30/100\n",
      "278/278 [==============================] - 0s - loss: 0.6580 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 31/100\n",
      "278/278 [==============================] - 0s - loss: 0.6575 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 32/100\n",
      "278/278 [==============================] - 0s - loss: 0.6575 - acc: 0.6151 - val_loss: 0.6704 - val_acc: 0.5833\n",
      "Epoch 33/100\n",
      "278/278 [==============================] - 0s - loss: 0.6573 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 34/100\n",
      "278/278 [==============================] - 0s - loss: 0.6576 - acc: 0.6151 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 35/100\n",
      "278/278 [==============================] - 0s - loss: 0.6573 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 36/100\n",
      "278/278 [==============================] - 0s - loss: 0.6572 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 37/100\n",
      "278/278 [==============================] - 0s - loss: 0.6572 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 38/100\n",
      "278/278 [==============================] - 0s - loss: 0.6570 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 39/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 40/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 41/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 42/100\n",
      "278/278 [==============================] - 0s - loss: 0.6570 - acc: 0.6151 - val_loss: 0.6701 - val_acc: 0.5833\n",
      "Epoch 43/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 44/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 45/100\n",
      "278/278 [==============================] - 0s - loss: 0.6566 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 46/100\n",
      "278/278 [==============================] - 0s - loss: 0.6569 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 47/100\n",
      "278/278 [==============================] - 0s - loss: 0.6565 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 48/100\n",
      "278/278 [==============================] - 0s - loss: 0.6564 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 49/100\n",
      "278/278 [==============================] - 0s - loss: 0.6565 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 50/100\n",
      "278/278 [==============================] - 0s - loss: 0.6566 - acc: 0.6151 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 51/100\n",
      "278/278 [==============================] - 0s - loss: 0.6563 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 52/100\n",
      "278/278 [==============================] - 0s - loss: 0.6563 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 53/100\n",
      "278/278 [==============================] - 0s - loss: 0.6562 - acc: 0.6151 - val_loss: 0.6713 - val_acc: 0.5833\n",
      "Epoch 54/100\n",
      "278/278 [==============================] - 0s - loss: 0.6563 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 55/100\n",
      "278/278 [==============================] - 0s - loss: 0.6560 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 56/100\n",
      "278/278 [==============================] - 0s - loss: 0.6561 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 57/100\n",
      "278/278 [==============================] - 0s - loss: 0.6559 - acc: 0.6151 - val_loss: 0.6711 - val_acc: 0.5833\n",
      "Epoch 58/100\n",
      "278/278 [==============================] - 0s - loss: 0.6560 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 59/100\n",
      "278/278 [==============================] - 0s - loss: 0.6559 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 60/100\n",
      "278/278 [==============================] - 0s - loss: 0.6561 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 61/100\n",
      "278/278 [==============================] - 0s - loss: 0.6561 - acc: 0.6151 - val_loss: 0.6708 - val_acc: 0.5833\n",
      "Epoch 62/100\n",
      "278/278 [==============================] - 0s - loss: 0.6559 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 63/100\n",
      "278/278 [==============================] - 0s - loss: 0.6558 - acc: 0.6151 - val_loss: 0.6706 - val_acc: 0.5833\n",
      "Epoch 64/100\n",
      "278/278 [==============================] - 0s - loss: 0.6559 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 65/100\n",
      "278/278 [==============================] - 0s - loss: 0.6558 - acc: 0.6151 - val_loss: 0.6706 - val_acc: 0.5833\n",
      "Epoch 66/100\n",
      "278/278 [==============================] - 0s - loss: 0.6557 - acc: 0.6151 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 67/100\n",
      "278/278 [==============================] - 0s - loss: 0.6557 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 68/100\n",
      "278/278 [==============================] - 0s - loss: 0.6556 - acc: 0.6151 - val_loss: 0.6706 - val_acc: 0.5833\n",
      "Epoch 69/100\n",
      "278/278 [==============================] - 0s - loss: 0.6554 - acc: 0.6151 - val_loss: 0.6706 - val_acc: 0.5833\n",
      "Epoch 70/100\n",
      "278/278 [==============================] - 0s - loss: 0.6553 - acc: 0.6151 - val_loss: 0.6700 - val_acc: 0.5833\n",
      "Epoch 71/100\n",
      "278/278 [==============================] - 0s - loss: 0.6556 - acc: 0.6151 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 72/100\n",
      "278/278 [==============================] - 0s - loss: 0.6556 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 73/100\n",
      "278/278 [==============================] - 0s - loss: 0.6553 - acc: 0.6151 - val_loss: 0.6702 - val_acc: 0.5833\n",
      "Epoch 74/100\n",
      "278/278 [==============================] - 0s - loss: 0.6556 - acc: 0.6151 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 75/100\n",
      "278/278 [==============================] - 0s - loss: 0.6552 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 76/100\n",
      "278/278 [==============================] - 0s - loss: 0.6552 - acc: 0.6151 - val_loss: 0.6701 - val_acc: 0.5833\n",
      "Epoch 77/100\n",
      "278/278 [==============================] - 0s - loss: 0.6553 - acc: 0.6151 - val_loss: 0.6699 - val_acc: 0.5833\n",
      "Epoch 78/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6706 - val_acc: 0.5833\n",
      "Epoch 79/100\n",
      "278/278 [==============================] - 0s - loss: 0.6552 - acc: 0.6151 - val_loss: 0.6702 - val_acc: 0.5833\n",
      "Epoch 80/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 81/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 82/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6704 - val_acc: 0.5833\n",
      "Epoch 83/100\n",
      "278/278 [==============================] - 0s - loss: 0.6547 - acc: 0.6151 - val_loss: 0.6694 - val_acc: 0.5833\n",
      "Epoch 84/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6698 - val_acc: 0.5833\n",
      "Epoch 85/100\n",
      "278/278 [==============================] - 0s - loss: 0.6548 - acc: 0.6151 - val_loss: 0.6697 - val_acc: 0.5833\n",
      "Epoch 86/100\n",
      "278/278 [==============================] - 0s - loss: 0.6547 - acc: 0.6151 - val_loss: 0.6693 - val_acc: 0.5833\n",
      "Epoch 87/100\n",
      "278/278 [==============================] - 0s - loss: 0.6550 - acc: 0.6151 - val_loss: 0.6700 - val_acc: 0.5833\n",
      "Epoch 88/100\n",
      "278/278 [==============================] - 0s - loss: 0.6547 - acc: 0.6151 - val_loss: 0.6699 - val_acc: 0.5833\n",
      "Epoch 89/100\n",
      "278/278 [==============================] - 0s - loss: 0.6546 - acc: 0.6151 - val_loss: 0.6698 - val_acc: 0.5833\n",
      "Epoch 90/100\n",
      "278/278 [==============================] - 0s - loss: 0.6547 - acc: 0.6151 - val_loss: 0.6697 - val_acc: 0.5833\n",
      "Epoch 91/100\n",
      "278/278 [==============================] - 0s - loss: 0.6546 - acc: 0.6151 - val_loss: 0.6700 - val_acc: 0.5833\n",
      "Epoch 92/100\n",
      "278/278 [==============================] - 0s - loss: 0.6546 - acc: 0.6151 - val_loss: 0.6690 - val_acc: 0.5833\n",
      "Epoch 93/100\n",
      "278/278 [==============================] - 0s - loss: 0.6545 - acc: 0.6151 - val_loss: 0.6686 - val_acc: 0.5833\n",
      "Epoch 94/100\n",
      "278/278 [==============================] - 0s - loss: 0.6546 - acc: 0.6151 - val_loss: 0.6689 - val_acc: 0.5833\n",
      "Epoch 95/100\n",
      "278/278 [==============================] - 0s - loss: 0.6545 - acc: 0.6151 - val_loss: 0.6692 - val_acc: 0.5833\n",
      "Epoch 96/100\n",
      "278/278 [==============================] - 0s - loss: 0.6543 - acc: 0.6151 - val_loss: 0.6692 - val_acc: 0.5833\n",
      "Epoch 97/100\n",
      "278/278 [==============================] - 0s - loss: 0.6545 - acc: 0.6151 - val_loss: 0.6692 - val_acc: 0.5833\n",
      "Epoch 98/100\n",
      "278/278 [==============================] - 0s - loss: 0.6543 - acc: 0.6151 - val_loss: 0.6692 - val_acc: 0.5833\n",
      "Epoch 99/100\n",
      "278/278 [==============================] - 0s - loss: 0.6542 - acc: 0.6151 - val_loss: 0.6693 - val_acc: 0.5833\n",
      "Epoch 100/100\n",
      "278/278 [==============================] - 0s - loss: 0.6542 - acc: 0.6151 - val_loss: 0.6692 - val_acc: 0.5833\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 50, validation_split = 0.3, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb4a4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['training','validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation\n",
    "- Keras model can be evaluated with evaluate() function\n",
    "- Evaluation results are contained in a list\n",
    "    - Doc (metrics): https://keras.io/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32/171 [====>.........................] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.62530872626611367, 0.67836257240228481]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)     # list of metric names the model is employing\n",
    "print(results)                 # actual figure of metrics computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss: ', 0.62530872626611367)\n",
      "('accuracy: ', 0.67836257240228481)\n"
     ]
    }
   ],
   "source": [
    "print('loss: ', results[0])\n",
    "print('accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
