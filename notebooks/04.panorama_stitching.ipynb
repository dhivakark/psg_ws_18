{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama Stitcthing of Images\n",
    "Multiple overlapping images of the same scene, combined into a single image, can yield amazing results. This tutorial will illustrate how to accomplish panorama stitching using OpenCV, from loading the images to cleverly stitching them together.\n",
    "\n",
    "### First things first\n",
    "Import NumPy,OpenCV and matplotlib, then define a utility function to compare multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "# Utility to display images   \n",
    "def display_side_by_side(images, **kwargs):\n",
    "    \"\"\"\n",
    "    Utility function to display images side by side.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image0, image1, image2, ... : ndarrray\n",
    "        Images to display.\n",
    "    labels : list\n",
    "        Labels for the different images.\n",
    "    \"\"\"\n",
    "    f, axes = plt.subplots(1, len(images), **kwargs)\n",
    "    axes = np.array(axes, ndmin=1)\n",
    "    \n",
    "    labels = kwargs.pop('labels', None)\n",
    "    if labels is None:\n",
    "        labels = [''] * len(images)\n",
    "    \n",
    "    for n, (image, label) in enumerate(zip(images, labels)):\n",
    "        axes[n].imshow(image, interpolation='nearest', cmap='gray')\n",
    "        axes[n].set_title(label)\n",
    "        axes[n].axis('off')\n",
    "    \n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data & Pre-processing\n",
    "\n",
    "This stage usually involves one or more of the following:\n",
    "* Resizing, often downscaling with fixed aspect ratio\n",
    "* Conversion to grayscale, as some feature descriptors are not defined for color images\n",
    "* Cropping to region(s) of interest\n",
    "\n",
    "For convenience our example data is already resized smaller, and we won't bother cropping. However, they are presently in color so coversion to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['../data/stitch_images/a1.jpg','../data/stitch_images/a2.jpg']\n",
    "images = [cv2.imread(each) for each in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_gray = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
    "#display_side_by_side(imgs_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Detection, Matching and Transform Estimation\n",
    "\n",
    "We need to estimate a projective transformation that relates these images together. The steps will be\n",
    "\n",
    "#### 1. Define one image as a _target_ or _destination_ image, which will remain anchored while the others are warped\n",
    "#### 2. Detect features in both images\n",
    "#### 3. Match features from one image against the features in the anchored image\n",
    "#### 4. Find the projective transform between them.\n",
    "\n",
    "We reuse code from our last workbook, use ORB features for finding matches and homography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(imgs_gray[0],None)\n",
    "kp2, des2 = orb.detectAndCompute(imgs_gray[1],None)\n",
    "#create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.knnMatch(des1,des2,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find good matches using lowe's ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find good matches\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "# cv2.drawMatches expects list of lists as matches\n",
    "img3 = cv2.drawMatches(imgs_gray[0],kp1,imgs_gray[1],kp2,good,None,flags=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the matches image\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Homography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(good)>4: #why 4?\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    #p12, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC,5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Warping\n",
    "Next, we produce the panorama itself. We must warp, or transform, one of image so it will properly align with the other image. Then merge other image into the warped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.warpPerspective(imgs_gray[1], p12, (imgs_gray[0].shape[1] + imgs_gray[1].shape[1], imgs_gray[0].shape[0]))\n",
    "#merge the two images\n",
    "result[0:imgs_gray[0].shape[0], 0:imgs_gray[0].shape[1]] = imgs_gray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the output image using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The end!\n"
     ]
    }
   ],
   "source": [
    "print('The end!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
